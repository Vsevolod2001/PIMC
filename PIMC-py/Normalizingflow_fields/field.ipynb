{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "55d2823b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3978be52-6e34-4e0a-b401-c56031d17d54",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import pytorch_lightning as pl\n",
    "from torch.utils.data import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c93eb4f7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from systems.Fields.scalar_field import Scalar_Field\n",
    "from systems.Fields.phi4 import Phi4\n",
    "from Data import MY_Dataset\n",
    "from flows.NormalizingFlow import NormalizingFlow    \n",
    "from lattice import Lattice\n",
    "\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "set_random_seed(42)\n",
    "\n",
    "\n",
    "latt = Lattice(np.array([8,8]),np.array([16,4]))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "23f2495c-14bc-49b4-8614-69d77ce8ef65",
   "metadata": {},
   "outputs": [],
   "source": [
    "normal_dist = torch.distributions.Normal(loc = torch.zeros(latt.total_nodes), scale = torch.ones(latt.total_nodes))\n",
    "DS=MY_Dataset(normal_dist,epoch_size=2**16)\n",
    "train_loader = DataLoader(DS, batch_size=2**12, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5401fb59",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        latent,\n",
    "        criterion,\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": 0.001,\"weight_decay\": 0}\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = criterion\n",
    "        self.latent = latent\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "    \"\"\"\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(\n",
    "            self.model.parameters(), **self.optimizer_kwargs\n",
    "        )\n",
    "        return optimizer\n",
    "    \"\"\"\n",
    "    \n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(self.model.parameters(), **self.optimizer_kwargs)\n",
    "        \n",
    "        scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
    "            optimizer=optimizer,\n",
    "            min_lr=1e-5,\n",
    "            factor=0.95,\n",
    "            mode=\"min\",\n",
    "            verbose=True,\n",
    "        )\n",
    "\n",
    "        lr_scheduler = {\n",
    "            \"scheduler\": scheduler,\n",
    "            \"interval\": \"epoch\",\n",
    "            \"monitor\": \"train_loss\",\n",
    "        }\n",
    "        return [optimizer], [lr_scheduler]\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        z = batch\n",
    "        latent_log_prob = torch.sum(self.latent.log_prob(z),(1,2))\n",
    "        \n",
    "        x, log_abs_det = self.model.g(z)\n",
    "        loss = self.loss(x,log_abs_det)\n",
    "        ess = self.loss.ESS(latent_log_prob,log_abs_det)\n",
    "\n",
    "        \n",
    "        sch = self.lr_schedulers()\n",
    "        sch.step(loss)\n",
    "        self.log('train_loss', loss,prog_bar=True)\n",
    "        self.log('ess',ess)\n",
    "        self.log('mean_x',torch.mean(x))\n",
    "        self.log('lr',sch.get_last_lr()[0])\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        #print(\"---------------------------end epoch---------------------------------\")\n",
    "        pass\n",
    "    \n",
    "    def on_validation_end(self) -> None:\n",
    "        if not self.automatic_optimization:\n",
    "            # Save a checkpoint of the model\n",
    "            ckpt_path = os.path.join(self.trainer.log_dir, 'checkpoints', 'ckpt.pt')\n",
    "            self.trainer.save_checkpoint(ckpt_path, weights_only=True)\n",
    "        return super().on_validation_end()\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "fa06713c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n"
     ]
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "rg = NormalizingFlow.config_and_init(n_flows = 4,num_hidden = 6,hidden_dim = 2 * latt.total_nodes,lattice=latt,ort=True)\n",
    "print(len(rg.flows))\n",
    "rg.save(\"./weights/first.pth\") "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "03b1a0a0-68d8-4471-93f0-1971b8b5a916",
   "metadata": {},
   "outputs": [],
   "source": [
    "rg = NormalizingFlow.load_model(\"./weights/first.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "33c48f58",
   "metadata": {},
   "outputs": [],
   "source": [
    "z = next(iter(train_loader))\n",
    "#print(z.shape)\n",
    "x = rg.g(z)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "95c33b71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_pair_split_masks_field(lattice,dir):\n",
    "    bigmask = list(range(lattice.total_nodes))\n",
    "    mask1 = list(filter(lambda x: x% (4 * lattice.n_nodes[dir] ** dir) < 2 * lattice.n_nodes[dir] ** dir,bigmask))       \n",
    "    mask2 = list(filter(lambda x: x%4>=2,bigmask))\n",
    "    return [mask1, mask2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "c944e112",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8\n",
      "[[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 32, 33, 34, 35, 36, 37, 38, 39, 40, 41, 42, 43, 44, 45, 46, 47], [2, 3, 6, 7, 10, 11, 14, 15, 18, 19, 22, 23, 26, 27, 30, 31, 34, 35, 38, 39, 42, 43, 46, 47, 50, 51, 54, 55, 58, 59, 62, 63]]\n"
     ]
    }
   ],
   "source": [
    "print(latt.n_nodes[0])\n",
    "mask=get_pair_split_masks_field(latt,1)\n",
    "print(mask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "512f9a56-e420-4e02-b861-4a4dfb0d7cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "C:\\Users\\SEVA1\\anaconda3\\Lib\\site-packages\\torch\\optim\\lr_scheduler.py:28: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
      "  warnings.warn(\"The verbose parameter is deprecated. Please use get_last_lr() \"\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | RGflows   | 3.0 M  | train\n",
      "1 | loss  | KL_with_S | 0      | train\n",
      "--------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.993    Total estimated model params size (MB)\n",
      "C:\\Users\\SEVA1\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:424: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "00074e79fc32432c8e7d389f3dcff935",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\SEVA1\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n",
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | RGflows   | 3.0 M  | train\n",
      "1 | loss  | KL_with_S | 0      | train\n",
      "--------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.993    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "988e6fffcb5e41bf86edd1d458e4a801",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | RGflows   | 3.0 M  | train\n",
      "1 | loss  | KL_with_S | 0      | train\n",
      "--------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.993    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "406d331dd71545878f1d2eb3a208bd94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "phi4.set_J_local(1,16)\n",
    "phi4.mass2 = 1\n",
    "for i in range(len(beta_arr)):\n",
    "    \n",
    "    phi4.beta = beta_arr[i]\n",
    "    pipeline = Pipeline(model = rg, \n",
    "                  latent = normal_dist ,\n",
    "                  criterion = phi4.get_KL(), \n",
    "                  optimizer_class=torch.optim.Adam,\n",
    "                  optimizer_kwargs={\"lr\": 0.001,\"weight_decay\":0.0})\n",
    "\n",
    "    trainer = pl.Trainer(\n",
    "        max_epochs = 15,\n",
    "        logger = TensorBoardLogger(save_dir=f\"./logs/field_phi4_beta\"),\n",
    "        num_sanity_val_steps = 0,\n",
    "        log_every_n_steps = 1,\n",
    "        enable_checkpointing = False,\n",
    "        accumulate_grad_batches = 1)\n",
    "\n",
    "    trainer.fit(model=pipeline, train_dataloaders=train_loader)\n",
    "    rg.save(\"./weights/field/phi4_beta_jloc/phi4_beta_jloc\"+str(beta_arr[i])+\".pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "a536cdfd",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params | Mode \n",
      "--------------------------------------------\n",
      "0 | model | RGflows   | 3.0 M  | train\n",
      "1 | loss  | KL_with_S | 0      | train\n",
      "--------------------------------------------\n",
      "3.0 M     Trainable params\n",
      "0         Non-trainable params\n",
      "3.0 M     Total params\n",
      "11.993    Total estimated model params size (MB)\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "999a94183aa442c8ab496f427081dbb9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "set_random_seed(42)\n",
    "scalar.set_J_local(4,16)\n",
    "phi4.set_J_local(4,16)\n",
    "pipeline=Pipeline(model=rg, \n",
    "                  latent = normal_dist ,\n",
    "                  criterion = phi4.get_KL(), \n",
    "                  optimizer_class=torch.optim.Adam,\n",
    "                  optimizer_kwargs={\"lr\": 0.001,\"weight_decay\":0.0})\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs = 2000,\n",
    "    logger = TensorBoardLogger(save_dir=f\"./logs/field\"),\n",
    "    num_sanity_val_steps = 0,\n",
    "    log_every_n_steps = 1,\n",
    "    enable_checkpointing = False,\n",
    "    accumulate_grad_batches = 1\n",
    ")\n",
    "\n",
    "trainer.fit(model=pipeline, train_dataloaders=train_loader)\n",
    "rg.save(\"./weights/field/phi4_beta_jloc/phi4_beta_jloc16.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "430cd760-acd0-4ce7-b978-1fd8d611c6d0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([1., 1., 1., 1., 1., 1., 1., 1.])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transforms import t_get_O\n",
    "mat = t_get_O(8)\n",
    "mat_t = torch.t(mat)\n",
    "ons = torch.einsum(\"sj,js->s\",mat,mat_t) \n",
    "print(ons)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43af8bc2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
