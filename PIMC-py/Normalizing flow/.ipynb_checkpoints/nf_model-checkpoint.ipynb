{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.distributions.distribution import Distribution"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# coupling layer\n",
    "based on\n",
    "\n",
    "https://lightning.ai/docs/pytorch/stable/notebooks/course_UvA-DL/09-normalizing-flows.html (VPN)\n",
    "\n",
    "https://sebastiancallh.github.io/post/affine-normalizing-flows/\n",
    "\n",
    "https://uvadlc-notebooks.readthedocs.io/en/latest/tutorial_notebooks/tutorial11/NF_image_modeling.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class AffineCouplingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        theta: nn.Module,\n",
    "        split: Callable[[torch.Tensor], Tuple[torch.Tensor, torch.Tensor]],\n",
    "        swap: int\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.split = split\n",
    "        self.swap=swap\n",
    "\n",
    "    def f(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"f : x -> z. The inverse of g.\"\"\"\n",
    "        x2, x1 = self.split(x,self.swap)\n",
    "        t, s = self.theta(x1)\n",
    "        z1, z2 = x1, x2 * torch.exp(s) + t \n",
    "        \"\"\"\n",
    "        z1 = x1\n",
    "        z2 = x2 * torch.exp(s) + t\n",
    "        \"\"\"\n",
    "        log_det = s.sum(-1) #-1 ???\n",
    "        return torch.cat((z1, z2), dim=-1), log_det\n",
    "\n",
    "    def g(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"g : z -> x. The inverse of f.\"\"\"\n",
    "        z1, z2 = self.split(z,self.swap)\n",
    "        t, s = self.theta(z1)\n",
    "        x1, x2 = z1, (z2 - t) * torch.exp(-s)\n",
    "        return torch.cat((x2, x1), dim=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class NormalizingFlow(nn.Module):\n",
    "    def __init__(self, latent: Distribution, flows: List[nn.Module]):\n",
    "        super().__init__()\n",
    "        self.latent = latent\n",
    "        self.flows = flows\n",
    "\n",
    "    def latent_log_prob(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        return torch.sum(self.latent.log_prob(z))\n",
    "\n",
    "    def latent_sample(self, num_samples: int = 1) -> torch.Tensor:\n",
    "        return self.latent.sample((num_samples,))                  \n",
    "\n",
    "    def sample(self, num_samples: int = 1) -> torch.Tensor:\n",
    "        \"\"\"Sample a new observation x by sampling z from\n",
    "        the latent distribution and pass through g.\"\"\"\n",
    "        return self.g(self.latent_sample(num_samples)) \n",
    "    \n",
    "\n",
    "    def f(self, x: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:  #forward\n",
    "        \"\"\"Maps observation x to latent variable z.\n",
    "        Additionally, computes the log determinant\n",
    "        of the Jacobian for this transformation.\n",
    "        Inveres of g.\"\"\"\n",
    "        z, sum_log_abs_det = x, torch.ones(x.size(0)).to(x.device)\n",
    "        for flow in self.flows:\n",
    "            z, log_abs_det = flow.f(z)\n",
    "            sum_log_abs_det += log_abs_det\n",
    "\n",
    "        return z, sum_log_abs_det\n",
    "\n",
    "    def g(self, z: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Maps latent variable z to observation x.\n",
    "        Inverse of f.\"\"\"\n",
    "        with torch.no_grad():\n",
    "            x = z\n",
    "            for flow in reversed(self.flows):\n",
    "                x = flow.g(x)\n",
    "\n",
    "            return x\n",
    "\n",
    "    def g_steps(self, z: torch.Tensor) -> List[torch.Tensor]:\n",
    "        \"\"\"Maps latent variable z to observation x\n",
    "        and stores intermediate results.\"\"\"\n",
    "        xs = [z]\n",
    "        for flow in reversed(self.flows):\n",
    "            xs.append(flow.g(xs[-1]))\n",
    "\n",
    "        return xs\n",
    "\n",
    "    def log_prob(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        \"\"\"Computes log p(x) using the change of variable formula.\"\"\"\n",
    "        z, log_abs_det = self.f(x)\n",
    "        return self.latent_log_prob(z) + log_abs_det\n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.flows)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class ThetaNetwork(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        in_dim: int,\n",
    "        out_dim: int,\n",
    "        num_hidden: int,\n",
    "        hidden_dim: int,\n",
    "        num_params: int,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.input = nn.Linear(in_dim, hidden_dim)\n",
    "        self.hidden = nn.ModuleList(\n",
    "            [nn.Linear(hidden_dim, hidden_dim) for _ in range(num_hidden)]\n",
    "        )\n",
    "\n",
    "        self.num_params = num_params\n",
    "        self.out_dim = out_dim\n",
    "        self.dims = nn.Linear(hidden_dim, out_dim * num_params)\n",
    "\n",
    "    def forward(self, x: torch.Tensor) -> torch.Tensor:\n",
    "        x = F.leaky_relu(self.input(x))\n",
    "        for h in self.hidden:\n",
    "            x = F.leaky_relu(h(x))\n",
    "\n",
    "        batch_params = self.dims(x).reshape(x.size(0), self.out_dim, -1) #???\n",
    "        params = batch_params.chunk(self.num_params, dim=-1) #???\n",
    "        return [p.squeeze(-1) for p in params]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def SplitFunc(x: torch.Tensor,swap: int) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    if swap==0:\n",
    "        return x[:,::2], x[:,1::2]\n",
    "    else: \n",
    "        return x[:,1::2], x[:,::2] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "from constants import N_nod\n",
    "def configure_theta():\n",
    "    theta=ThetaNetwork(\n",
    "                in_dim = N_nod//2,\n",
    "                out_dim = N_nod//2,\n",
    "                num_hidden = 4,  #2 to 6\n",
    "                hidden_dim =100 , #100-1024\n",
    "                num_params = 2)\n",
    "    return theta\n",
    "def configure_flows(n_flows):  # n_flows=8,...,12\n",
    "    flows=[]\n",
    "    for i in range(n_flows):\n",
    "        flows.append(AffineCouplingLayer(configure_theta(),split=SplitFunc,swap=i%2))\n",
    "    flows = nn.ModuleList(flows)\n",
    "    return flows \n",
    "nf=NormalizingFlow(latent=torch.distributions.Normal(loc=0.0, scale=1.),\n",
    "                  flows=configure_flows(8))\n",
    "                  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Pipeline(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": 0.001},\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(\n",
    "            self.model.parameters(), **self.optimizer_kwargs\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        print(\"step\")\n",
    "        z = batch\n",
    "        x = self.model.g(z)\n",
    "        print(\"x:\",x.shape)\n",
    "        LP=self.model.log_prob(x)\n",
    "        loss = self.loss(x,LP)\n",
    "        self.log('train_loss', loss)\n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        print(\"epoch end\")\n",
    "        pass\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "\n",
      "  | Name  | Type            | Params\n",
      "------------------------------------------\n",
      "0 | model | NormalizingFlow | 336 K \n",
      "1 | loss  | KL_with_S       | 0     \n",
      "------------------------------------------\n",
      "336 K     Trainable params\n",
      "0         Non-trainable params\n",
      "336 K     Total params\n",
      "1.344     Total estimated model params size (MB)\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:133: UserWarning: The dataloader, train_dataloader, does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` (try 4 which is the number of cpus on this machine) in the `DataLoader` init to improve performance.\n",
      "  f\"The dataloader, {name}, does not have many workers which may be a bottleneck.\"\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\pytorch_lightning\\trainer\\data_loading.py:433: UserWarning: The number of training samples (1) is smaller than the logging interval Trainer(log_every_n_steps=50). Set a lower value for log_every_n_steps if you want to see logs for the training epoch.\n",
      "  f\"The number of training samples ({self.num_training_batches}) is smaller than the logging interval\"\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b75ae4e95ce649d0a1e39cc62ba4b4ce",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "A Jupyter Widget"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "step\n",
      "x: torch.Size([256, 10])\n",
      "epoch end\n",
      "step\n",
      "x: torch.Size([256, 10])\n",
      "epoch end\n",
      "step\n",
      "x: torch.Size([256, 10])\n",
      "epoch end\n",
      "step\n",
      "x: torch.Size([256, 10])\n",
      "epoch end\n",
      "step\n",
      "x: torch.Size([256, 10])\n",
      "epoch end\n"
     ]
    }
   ],
   "source": [
    "from LOSS import KL_osc\n",
    "from Data import train_loader\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "pipeline=Pipeline(model=nf,criterion=KL_osc)\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=5,\n",
    "    logger=TensorBoardLogger(save_dir=f\"logs/nf\"),\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model=pipeline, train_dataloaders=train_loader)\n",
    "torch.save(nf.state_dict(), \"model_weights1.pth\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
