{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2c367363",
   "metadata": {
    "execution": {
     "iopub.status.busy": "2024-01-24T16:44:28.821474960Z",
     "iopub.status.idle": "2024-01-24T16:44:28.824855025Z",
     "shell.execute_reply": "2024-01-24T16:44:28.821419169Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from typing import Callable, List, Tuple\n",
    "\n",
    "import pytorch_lightning as pl\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn\n",
    "from torch.distributions.distribution import Distribution\n",
    "from pytorch_lightning.loggers import TensorBoardLogger\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ffdb9efa",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class L1(nn.Linear):\n",
    "    def __init__(self,n):\n",
    "        super().__init__(n*(n-1)//2,1,bias=False)\n",
    "        self.n=n\n",
    "        self.mask2d=torch.zeros((n*(n-1) // 2),dtype=int)\n",
    "        for i in range(1,n):\n",
    "            for j in range(i):\n",
    "                self.mask2d[i*(i-1)//2+j]=i*n+j\n",
    "        self.d_ind=[(n+1)*k for k in range(n)]\n",
    "        self.ones=torch.ones((n))\n",
    "    \n",
    "    def anti_flatten(self):\n",
    "        n=self.n\n",
    "        L=torch.zeros((n**2)).to(device)\n",
    "        L[self.mask2d]=self.weight.to(device)\n",
    "        L[self.d_ind]=self.ones\n",
    "        L=torch.reshape(L,(n,n)).to(device)\n",
    "        return L        \n",
    "            \n",
    "    def log_abs_det(self):\n",
    "        return 0\n",
    "    \n",
    "    def forward(self,x):\n",
    "        Lwt=torch.t(self.anti_flatten())\n",
    "        return torch.matmul(x,Lwt)\n",
    "    \n",
    "    def g(self,z):\n",
    "        return self.forward(z), torch.zeros((z.shape[0]))\n",
    "        \n",
    "    def adj(self,mat):\n",
    "        Lw=self.anti_flatten()\n",
    "        Lwt=torch.t(Lw)\n",
    "        D=torch.matmul(Lwt,torch.matmul(mat,Lw))\n",
    "        return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "02681dbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class D(nn.Linear):\n",
    "    def __init__(self,n):\n",
    "        super().__init__(n,1,bias=False)\n",
    "        self.n=n\n",
    "        self.d_ind=[(n+1)*k for k in range(n)]\n",
    "    \n",
    "    def anti_flatten(self):\n",
    "        n=self.n\n",
    "        D=torch.zeros((n**2)).to(device)\n",
    "        D[self.d_ind]=self.weight.to(device)\n",
    "        D=torch.reshape(D,(n,n)).to(device)\n",
    "        return D        \n",
    "            \n",
    "    def log_abs_det(self):\n",
    "        ABS=torch.abs(self.weight)\n",
    "        l=torch.log(ABS)\n",
    "        lad=torch.sum(l)\n",
    "        return lad\n",
    "    \n",
    "    def forward(self,x):\n",
    "        D=self.anti_flatten()\n",
    "        return torch.matmul(x,D)\n",
    "    \n",
    "    def g(self,z):\n",
    "        lad=self.log_abs_det()\n",
    "        return self.forward(z), lad * torch.ones((z.shape[0]))\n",
    "        \n",
    "    def adj(self,mat):\n",
    "        Lw=self.anti_flatten()\n",
    "        Lwt=torch.t(Lw)\n",
    "        D=torch.matmul(Lwt,torch.matmul(mat,Lw))\n",
    "        return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "de40e85b",
   "metadata": {},
   "outputs": [],
   "source": [
    "m=L1(10)\n",
    "#print(m.anti_flatten())\n",
    "d=D(10)\n",
    "#print(d.anti_flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a0c95929",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[-0.1542,  0.0047,  0.0486,  0.0127,  0.1100, -0.0590, -0.0167, -0.0297,\n",
      "         -0.2712, -0.2027],\n",
      "        [-0.0516,  0.0984,  0.0572,  0.1036,  0.1347, -0.1432, -0.0105, -0.0316,\n",
      "         -0.2417, -0.0762],\n",
      "        [-0.1121,  0.0695,  0.0009,  0.0702,  0.0379, -0.0906, -0.0063, -0.0245,\n",
      "         -0.2078, -0.1494],\n",
      "        [-0.1291,  0.1591,  0.0083,  0.1512,  0.0816, -0.1365, -0.0169, -0.0723,\n",
      "         -0.1708, -0.0077],\n",
      "        [-0.1400,  0.0687,  0.0496,  0.0928,  0.0417, -0.0191, -0.0006, -0.0902,\n",
      "         -0.1710, -0.1761],\n",
      "        [-0.1289,  0.0262,  0.0193,  0.0812,  0.0394, -0.0555, -0.0045, -0.1020,\n",
      "         -0.1071, -0.0184],\n",
      "        [-0.0656,  0.1773,  0.0030,  0.0091,  0.0302, -0.1210, -0.0071, -0.0334,\n",
      "         -0.0228, -0.1690],\n",
      "        [-0.1687,  0.1891,  0.0626,  0.1293,  0.1479, -0.1799, -0.0174, -0.0826,\n",
      "         -0.2242, -0.1378],\n",
      "        [-0.0242,  0.1924,  0.0221,  0.0611,  0.0257, -0.0664, -0.0079, -0.0229,\n",
      "         -0.2622, -0.2476],\n",
      "        [-0.0376,  0.2065,  0.0457,  0.1389,  0.1234, -0.0254, -0.0080, -0.0993,\n",
      "         -0.2751, -0.2978]], grad_fn=<MmBackward0>) tensor([-19.8657, -19.8657, -19.8657, -19.8657, -19.8657, -19.8657, -19.8657,\n",
      "        -19.8657, -19.8657, -19.8657], grad_fn=<MulBackward0>)\n"
     ]
    }
   ],
   "source": [
    "z=torch.rand((10,10))\n",
    "x, lad= m.g(z)\n",
    "x, lad= d.g(z)\n",
    "print(x, lad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bb50635",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:57:44.641553Z",
     "iopub.status.busy": "2024-01-21T16:57:44.640468Z",
     "iopub.status.idle": "2024-01-21T16:57:44.663958Z",
     "shell.execute_reply": "2024-01-21T16:57:44.662825Z",
     "shell.execute_reply.started": "2024-01-21T16:57:44.641502Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "import numpy as np\n",
    "\n",
    "class L(nn.Linear):\n",
    "    def __init__(self,n):\n",
    "        super().__init__(n*(n+1)//2,1,bias=False)\n",
    "        self.n=n\n",
    "        self.diag_mask=torch.tensor([ (k+1) * (k+2) // 2 - 1 for k in range(n)])\n",
    "        self.mask2d=torch.zeros((n*(n+1) // 2),dtype=int)\n",
    "        for i in range(n):\n",
    "            for j in range(i+1):\n",
    "                self.mask2d[i*(i+1)//2+j]=i*n+j\n",
    "    \n",
    "    def anti_flatten(self):\n",
    "        n=self.n\n",
    "        L=torch.zeros((n**2)).to(device)\n",
    "        L[self.mask2d]=self.weight.to(device) \n",
    "        L=torch.reshape(L,(n,n)).to(device)\n",
    "        return L        \n",
    "            \n",
    "    def log_abs_det(self):\n",
    "        diag=self.weight[0][self.diag_mask]\n",
    "        la=torch.log(torch.abs(diag))\n",
    "        lad=torch.sum(la)\n",
    "        return lad\n",
    "    \n",
    "    def forward(self,x):\n",
    "        Lwt=torch.t(self.anti_flatten())\n",
    "        return torch.matmul(x,Lwt)\n",
    "        \n",
    "    def adj(self,mat):\n",
    "        Lw=self.anti_flatten()\n",
    "        Lwt=torch.t(Lw)\n",
    "        D=torch.matmul(Lwt,torch.matmul(mat,Lw))\n",
    "        return D\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4177d0f7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n"
     ]
    }
   ],
   "source": [
    "from NFandist import get_O, get_A\n",
    "from NFconstants import N_nod, beta\n",
    "O=(torch.tensor(get_O(N_nod)).float()).to(device)\n",
    "Ot=torch.t(O)\n",
    "print(Ot.requires_grad)\n",
    "\n",
    "A=(torch.tensor(get_A(N_nod,beta)).float()).to(device)\n",
    "I=(torch.eye(N_nod)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a112bc9b",
   "metadata": {},
   "outputs": [],
   "source": [
    "class simple_nf(nn.Module):\n",
    "     \n",
    "    def __init__(self,layer,ort=True):\n",
    "        super().__init__()\n",
    "        self.layer=layer\n",
    "        self.ort=ort\n",
    "\n",
    "    def log_abs_det(self):\n",
    "        lad=self.layer.log_abs_det()\n",
    "        return lad\n",
    "    \n",
    "    def forward(self,x):\n",
    "        y=self.layer(x)\n",
    "        if self.ort:\n",
    "            y=torch.matmul(y,Ot)\n",
    "        return y\n",
    "        \n",
    "    def metric(self):\n",
    "        with torch.no_grad():\n",
    "            if self.ort:\n",
    "                B=torch.matmul(Ot,torch.matmul(A,O))\n",
    "            else:\n",
    "                B=A\n",
    "            met=torch.linalg.matrix_norm(self.layer.adj(B)-I)\n",
    "        return met  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1ab3fa53",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:57:45.696455Z",
     "iopub.status.busy": "2024-01-21T16:57:45.695166Z",
     "iopub.status.idle": "2024-01-21T16:57:46.086456Z",
     "shell.execute_reply": "2024-01-21T16:57:46.085371Z",
     "shell.execute_reply.started": "2024-01-21T16:57:45.696400Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from NFconstants import N_nod, beta , N_traj\n",
    "from NFandist import get_A, get_C\n",
    "A=(torch.tensor(get_A(N_nod,beta)).float()).to(device)\n",
    "I=(torch.eye(N_nod)).to(device)\n",
    "def set_random_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    np.random.seed(seed)\n",
    "\n",
    "def A_I(model):\n",
    "    with torch.no_grad():\n",
    "        A_D=model.adj(A)\n",
    "    return torch.linalg.matrix_norm(A_D-I)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b66a3749",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:57:46.453500Z",
     "iopub.status.busy": "2024-01-21T16:57:46.452562Z",
     "iopub.status.idle": "2024-01-21T16:57:46.467139Z",
     "shell.execute_reply": "2024-01-21T16:57:46.466118Z",
     "shell.execute_reply.started": "2024-01-21T16:57:46.453455Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Cheatloss(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Cheatloss, self).__init__()\n",
    "\n",
    "    def forward(self, model,lad):\n",
    "        A_D=model.adj(A)\n",
    "        loss=0.5*torch.trace(A_D)-lad\n",
    "        return loss\n",
    "CL=Cheatloss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d85bbd7b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:57:48.022398Z",
     "iopub.status.busy": "2024-01-21T16:57:48.021387Z",
     "iopub.status.idle": "2024-01-21T16:57:48.038714Z",
     "shell.execute_reply": "2024-01-21T16:57:48.037452Z",
     "shell.execute_reply.started": "2024-01-21T16:57:48.022345Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "class Pipeline(pl.LightningModule):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        criterion,\n",
    "        optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": 0.001,\"weight_decay\": 0.01}\n",
    "    ) -> None:\n",
    "        super().__init__()\n",
    "        self.model = model\n",
    "        self.loss = criterion\n",
    "        self.optimizer_class = optimizer_class\n",
    "        self.optimizer_kwargs = optimizer_kwargs\n",
    "\n",
    "\n",
    "    def configure_optimizers(self):\n",
    "        optimizer = self.optimizer_class(\n",
    "            self.model.parameters(), **self.optimizer_kwargs\n",
    "        )\n",
    "        return optimizer\n",
    "\n",
    "    def training_step(self, batch, batch_idx):\n",
    "        z = batch\n",
    "        x, log_abs_det = self.model(z), self.model.log_abs_det()\n",
    "        #log_abs_det=self.model.log_abs_det()\n",
    "        loss = self.loss(x,log_abs_det)\n",
    "        self.log('train_loss', loss, prog_bar=True)\n",
    "        \n",
    "        return loss\n",
    "\n",
    "\n",
    "    def on_train_epoch_end(self):\n",
    "        metric=self.model.metric()\n",
    "        self.log(\"metric\",metric, prog_bar=True)\n",
    "        #self.log(\"metric\",A_I(self.model), prog_bar=True)\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "2b5b0325",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T16:57:49.272063Z",
     "iopub.status.busy": "2024-01-21T16:57:49.270976Z",
     "iopub.status.idle": "2024-01-21T17:15:05.447242Z",
     "shell.execute_reply": "2024-01-21T17:15:05.445798Z",
     "shell.execute_reply.started": "2024-01-21T16:57:49.272013Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "GPU available: False, used: False\n",
      "TPU available: False, using: 0 TPU cores\n",
      "IPU available: False, using: 0 IPUs\n",
      "HPU available: False, using: 0 HPUs\n",
      "\n",
      "  | Name  | Type      | Params\n",
      "------------------------------------\n",
      "0 | model | simple_nf | 16    \n",
      "1 | loss  | KL_with_S | 0     \n",
      "------------------------------------\n",
      "16        Trainable params\n",
      "0         Non-trainable params\n",
      "16        Total params\n",
      "0.000     Total estimated model params size (MB)\n",
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\connectors\\data_connector.py:441: The 'train_dataloader' does not have many workers which may be a bottleneck. Consider increasing the value of the `num_workers` argument` to `num_workers=3` in the `DataLoader` to improve performance.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6e58ac411680428aae58c75988af783a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Training: |                                                                                      | 0/? [00:00<…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\anaconda3\\Lib\\site-packages\\pytorch_lightning\\trainer\\call.py:54: Detected KeyboardInterrupt, attempting graceful shutdown...\n"
     ]
    }
   ],
   "source": [
    "from LOSS import KL_osc\n",
    "from Data import train_loader\n",
    "\n",
    "set_random_seed(42)\n",
    "snf=simple_nf(D(N_nod),ort=True)\n",
    "pipeline=Pipeline(model=snf,criterion=KL_osc, optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": 0.001, \"weight_decay\": 0.01})\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=1000,\n",
    "    logger=TensorBoardLogger(save_dir=f\"logs/nf\"),\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model=pipeline, train_dataloaders=train_loader)\n",
    "torch.save(snf.state_dict(), \"L_layer_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "836f35aa",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T17:15:05.450939Z",
     "iopub.status.busy": "2024-01-21T17:15:05.450137Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Error(s) in loading state_dict for simple_nf:\n\tMissing key(s) in state_dict: \"layer.weight\". \n\tUnexpected key(s) in state_dict: \"weight\". ",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-56-11624d6ca6fe>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msimple_nf\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mL_layer\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;31m#model=L_layer\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload_state_dict\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"L_layer_weights.pth\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m pipeline=Pipeline(model=model,criterion=KL_osc, optimizer_class=torch.optim.Adam,\n\u001b[0;32m      6\u001b[0m         optimizer_kwargs={\"lr\": 0.001,\"weight_decay\": 0})\n",
      "\u001b[1;32mC:\\ProgramData\\Anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py\u001b[0m in \u001b[0;36mload_state_dict\u001b[1;34m(self, state_dict, strict)\u001b[0m\n\u001b[0;32m   1481\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0merror_msgs\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1482\u001b[0m             raise RuntimeError('Error(s) in loading state_dict for {}:\\n\\t{}'.format(\n\u001b[1;32m-> 1483\u001b[1;33m                                self.__class__.__name__, \"\\n\\t\".join(error_msgs)))\n\u001b[0m\u001b[0;32m   1484\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0m_IncompatibleKeys\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmissing_keys\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0munexpected_keys\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1485\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Error(s) in loading state_dict for simple_nf:\n\tMissing key(s) in state_dict: \"layer.weight\". \n\tUnexpected key(s) in state_dict: \"weight\". "
     ]
    }
   ],
   "source": [
    "L_layer=L(N_nod)\n",
    "model=simple_nf(L_layer,ort=True)\n",
    "#model=L_layer\n",
    "model.load_state_dict(torch.load(\"L_layer_weights.pth\"))\n",
    "pipeline=Pipeline(model=model,criterion=KL_osc, optimizer_class=torch.optim.Adam,\n",
    "        optimizer_kwargs={\"lr\": 0.001,\"weight_decay\": 0})\n",
    "\n",
    "trainer = pl.Trainer(\n",
    "    max_epochs=2000,\n",
    "    logger=TensorBoardLogger(save_dir=f\"logs/nf\"),\n",
    "    num_sanity_val_steps=0,\n",
    ")\n",
    "\n",
    "trainer.fit(model=pipeline, train_dataloaders=train_loader)\n",
    "torch.save(model.state_dict(), \"L_layer_weights.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f837ce60",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NFandist import get_T\n",
    "T=torch.tensor(get_T(N_nod)).float()\n",
    "def G(X,n_p=N_nod):\n",
    "    G=np.zeros((n_p))\n",
    "    Y=X.clone()\n",
    "    Xt=torch.t(X)\n",
    "    for s in range(n_p):\n",
    "        G[s]=torch.trace(torch.matmul(Y,Xt))\n",
    "        Y=torch.matmul(Y,T)\n",
    "    return G/(N_traj*N_nod)\n",
    "g_nf=G(trajs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "817d4670",
   "metadata": {},
   "outputs": [],
   "source": [
    "from NFandist import calc_G\n",
    "from NFconstants import N_nod, N_traj, NG_points,beta\n",
    "g_osc=calc_G(N_nod,beta,N_nod)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "775f8d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#import sys\n",
    "#sys.path.append('../')\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from NFconstants import N_nod, N_traj, NG_points,beta\n",
    "#from Value import G\n",
    "#import ensemble\n",
    "#from NFoscillator import basic_oscillator\n",
    "#from time import time\n",
    "#from NFandist import calc_G\n",
    "\n",
    "\"\"\"\n",
    "ens_nf=ensemble.ensemble.load(\"nf_ensemble.txt\",basic_oscillator)\n",
    "g_nf=np.vstack(ensemble.ensemble.Vaverage_and_sigma(ens_nf,G))\n",
    "g_nf=g_nf.transpose()[0]\n",
    "\"\"\"\n",
    "\n",
    "g=g_ur\n",
    "print(len(g))\n",
    "fig=plt.figure()\n",
    "MCMC_list=np.arange(len(g))/len(g)\n",
    "NF_list=np.arange(len(g_nf))/len(g_nf)\n",
    "plt.scatter(MCMC_list,g)\n",
    "plt.scatter(NF_list,g_nf)\n",
    "plt.legend([\"MCMC\",\"normalizing flow\"])\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "ad5b7542",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2024-01-21T14:46:43.974083Z",
     "iopub.status.busy": "2024-01-21T14:46:43.972858Z",
     "iopub.status.idle": "2024-01-21T14:46:44.081082Z",
     "shell.execute_reply": "2024-01-21T14:46:44.080032Z",
     "shell.execute_reply.started": "2024-01-21T14:46:43.974037Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 9.7450e-01,  6.8725e-03,  1.6029e-03,  ...,  2.9996e-02,\n",
      "          1.2987e-02, -1.0077e-02],\n",
      "        [ 6.8728e-03,  1.0180e+00, -1.9002e-02,  ...,  4.0149e-04,\n",
      "         -1.8119e-02,  1.3663e-03],\n",
      "        [ 1.6030e-03, -1.9002e-02,  9.8187e-01,  ...,  1.0302e-02,\n",
      "         -7.3258e-03,  9.5749e-03],\n",
      "        ...,\n",
      "        [ 2.9996e-02,  4.0149e-04,  1.0302e-02,  ...,  9.7388e-01,\n",
      "         -4.3421e-03, -1.0112e-02],\n",
      "        [ 1.2987e-02, -1.8119e-02, -7.3258e-03,  ..., -4.3421e-03,\n",
      "          1.0155e+00, -2.4950e-02],\n",
      "        [-1.0077e-02,  1.3663e-03,  9.5749e-03,  ..., -1.0112e-02,\n",
      "         -2.4950e-02,  1.0105e+00]], grad_fn=<MmBackward0>)\n"
     ]
    }
   ],
   "source": [
    "L_layer=L(N_nod)\n",
    "L_layer.load_state_dict(torch.load(\"L_layer_weights.pth\"))\n",
    "print(L_layer.adj(A))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "ca4c84d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[ 1.0000e+00,  5.8752e-08, -1.3914e-08,  ...,  2.2872e-08,\n",
      "          2.5498e-07,  0.0000e+00],\n",
      "        [ 4.0992e-08,  1.0000e+00, -2.7618e-08,  ...,  2.2800e-08,\n",
      "         -2.4269e-08,  2.8902e-08],\n",
      "        [-5.8108e-10, -9.3866e-08,  1.0000e+00,  ...,  4.4837e-09,\n",
      "         -4.7684e-07, -5.5179e-08],\n",
      "        ...,\n",
      "        [-1.2623e-09,  5.3057e-09, -3.6523e-10,  ...,  1.0000e+00,\n",
      "         -3.5937e-09,  1.9324e-08],\n",
      "        [-7.7986e-10, -1.9018e-10,  1.1782e-10,  ...,  5.4074e-10,\n",
      "          1.0000e+00, -2.6425e-09],\n",
      "        [ 7.4506e-09, -1.3039e-08, -2.9977e-09,  ..., -1.6764e-08,\n",
      "         -5.8208e-09,  1.0000e+00]])\n",
      "tensor([[ 2.0000e-01, -7.7642e-09, -1.9571e-09,  ..., -1.1199e-07,\n",
      "         -3.6891e-07,  0.0000e+00],\n",
      "        [-6.7893e-10,  2.7885e-01, -2.3201e-08,  ...,  2.8581e-08,\n",
      "          3.0639e-09,  3.8130e-08],\n",
      "        [-4.9183e-10, -5.3233e-09,  2.7885e-01,  ..., -8.5684e-10,\n",
      "         -9.5367e-07, -5.2030e-07],\n",
      "        ...,\n",
      "        [ 3.9386e-09, -8.6246e-11, -2.1778e-09,  ...,  2.0121e+01,\n",
      "         -7.6323e-08, -8.9375e-08],\n",
      "        [ 9.0515e-10,  3.1814e-11, -8.4611e-11,  ..., -3.0659e-08,\n",
      "          2.0121e+01, -1.7668e-07],\n",
      "        [ 7.4506e-09, -3.8650e-08, -2.2585e-08,  ...,  1.1921e-07,\n",
      "          7.4506e-09,  2.0200e+01]])\n",
      "tensor(2.4544e-06)\n"
     ]
    }
   ],
   "source": [
    "C=(torch.tensor(get_C(N_nod,beta)).float()).to(device)\n",
    "Ct=torch.t(C)\n",
    "A_e=torch.matmul(Ct,torch.matmul(A,C))\n",
    "print(A_e)\n",
    "A_D=torch.matmul(Ot,torch.matmul(A,O))\n",
    "print(A_D)\n",
    "print(torch.linalg.matrix_norm(A_e-I))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e94e763",
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
