{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38ffa189",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_multiO(m,N):\n",
    "    omasks = config_RG_masks(m,N)\n",
    "    mO=np.zeros((N,N))\n",
    "    for i in range(len(omasks)):\n",
    "        dim=len(omasks[i])\n",
    "        Pt=np.zeros((N,dim))\n",
    "        Pt[omasks[i]] = np.eye(dim)\n",
    "        P=np.transpose(Pt)\n",
    "        O=get_O(dim)\n",
    "        mO+=np.dot(Pt,np.dot(O,P))\n",
    "    return mO\n",
    "mO=get_multiO(3,N_nod)\n",
    "mOt=O=(torch.tensor(np.transpose(mO)).float()).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97cf3e20",
   "metadata": {},
   "outputs": [],
   "source": [
    "def getproj(mask,N=N_nod):\n",
    "    dim = len(mask)\n",
    "    Pt=torch.zeros((N,dim))\n",
    "    Pt[mask] = torch.eye(dim)\n",
    "    return Pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "2bed51ef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1.  0.  0.  0. ]\n",
      " [0.5 0.5 0.  0. ]\n",
      " [0.  1.  0.  0. ]\n",
      " [0.  0.5 0.5 0. ]\n",
      " [0.  0.  1.  0. ]\n",
      " [0.  0.  0.5 0.5]\n",
      " [0.  0.  0.  1. ]\n",
      " [0.5 0.  0.  0.5]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "def bininter(dim):\n",
    "    P = np.zeros((2*dim,dim))\n",
    "    mask = [2*i for i in range(dim)]\n",
    "    mask1 = [2*i+1 for i in range(dim)]\n",
    "    P[mask] = np.eye(dim)\n",
    "    P[mask1] = 0.5 * np.eye(dim) + 0.5 * np.roll(np.eye(dim),1,axis=1)\n",
    "    return P\n",
    "print(bininter(4))    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36418847",
   "metadata": {},
   "outputs": [],
   "source": [
    "class KL_mix(nn.Module):\n",
    "    \n",
    "    def __init__(self,KL1,KL2,alpha=0.5):\n",
    "        super().__init__()\n",
    "        self.KL1 = KL1\n",
    "        self.KL2 = KL2\n",
    "        self.alpha = alpha\n",
    "    \n",
    "    def forward(self,x,log_abs_det):\n",
    "        n_nod = x.shape[1]\n",
    "        N2 = self.KL2.n_nod\n",
    "        step = n_nod//N2\n",
    "        x1 = x[:,::step]\n",
    "        loss = (1-self.alpha) * self.KL1(x,log_abs_det) + self.alpha * self.KL2(x1,log_abs_det) \n",
    "        return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caf67411",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CubicCouplingLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        theta,\n",
    "        split,\n",
    "        swap: int,\n",
    "        reg=1\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.theta = theta\n",
    "        self.split = split\n",
    "        self.swap = swap\n",
    "        self.reg=reg\n",
    "        \n",
    "    def g(self, z: torch.Tensor,params=torch.tensor([])) -> torch.Tensor:\n",
    "        \"\"\"g : z -> x. The inverse of f.\"\"\"\n",
    "        mask1=self.split[self.swap]\n",
    "        mask2=self.split[(self.swap+1)%2]\n",
    "        z1, z2 = z[:,mask1], z[:,mask2]\n",
    "        z1=torch.cat((z1,params),dim=-1)\n",
    "        t, s = self.theta(z1)\n",
    "        x2 = z2 + torch.exp(s-self.reg) * (z2) ** 3 + t\n",
    "        log_det = (torch.log(1+3 * torch.exp(s-self.reg) * (z2) ** 2)).sum(-1)\n",
    "        z[:,mask2]=x2\n",
    "        return z, log_det    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e2d9c4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class D(nn.Linear):\n",
    "    def __init__(self,n):\n",
    "        super().__init__(n,1,bias=False)\n",
    "        self.n=n\n",
    "        self.d_ind=[(n+1)*k for k in range(n)]\n",
    "    \n",
    "    def anti_flatten(self):\n",
    "        n=self.n\n",
    "        D=torch.zeros((n**2)).to(device)\n",
    "        D[self.d_ind]=self.weight.to(device)\n",
    "        D=torch.reshape(D,(n,n)).to(device)\n",
    "        return D        \n",
    "            \n",
    "    def log_abs_det(self):\n",
    "        ABS=torch.abs(self.weight)\n",
    "        l=torch.log(ABS)\n",
    "        lad=torch.sum(l)\n",
    "        return lad\n",
    "    \n",
    "    def forward(self,x):\n",
    "        D=(self.anti_flatten()).to(x.device)\n",
    "        return torch.matmul(x,D)\n",
    "    \n",
    "    def g(self,z,params):\n",
    "        lad=self.log_abs_det()\n",
    "        return self.forward(z),  lad * torch.ones((z.shape[0])).to(z.device)\n",
    "        \n",
    "    def adj(self,mat):\n",
    "        Lw=self.anti_flatten()\n",
    "        Lwt=torch.t(Lw)\n",
    "        D=torch.matmul(Lwt,torch.matmul(mat,Lw))\n",
    "        return D\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
